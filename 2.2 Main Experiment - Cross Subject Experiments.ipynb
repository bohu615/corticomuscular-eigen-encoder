{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e489684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "subject = np.load('../run_experiments_ssl_signal/subject.npy')\n",
    "label = np.load('../run_experiments_ssl_signal/label.npy')\n",
    "new_label = np.load('../run_experiments_ssl_signal/new_label.npy')\n",
    "eeg_data = np.load('../run_experiments_ssl_signal/eeg_data.npy')\n",
    "emg_data = np.load('../run_experiments_ssl_signal/emg_data.npy')\n",
    "\n",
    "subject = torch.from_numpy(subject).long()\n",
    "label = torch.from_numpy(label).long()\n",
    "new_label = torch.from_numpy(new_label).long()\n",
    "eeg_data = torch.from_numpy(eeg_data).float()\n",
    "emg_data = torch.from_numpy(emg_data).float()\n",
    "\n",
    "SAMPLE_X = eeg_data\n",
    "SAMPLE_Y = emg_data\n",
    "label_tensor = new_label\n",
    "\n",
    "# Generate Data Cross Subject\n",
    "\n",
    "train_idx = [np.where(subject == k)[0] for k in range(0, 20)]\n",
    "test_idx = [np.where(subject == k)[0] for k in range(20, 25)]\n",
    "\n",
    "train_idx = np.concatenate(train_idx)\n",
    "test_idx = np.concatenate(test_idx)\n",
    "\n",
    "train_major_label, test_major_label = new_label[train_idx], new_label[test_idx]\n",
    "train_subtle_label, test_subtle_label = label[train_idx], label[test_idx]\n",
    "train_subj_label, test_subj_label = subject[train_idx], subject[test_idx]\n",
    "\n",
    "train_X, test_X = SAMPLE_X[train_idx], SAMPLE_X[test_idx]\n",
    "train_Y, test_Y = SAMPLE_Y[train_idx], SAMPLE_Y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f601322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Subset, Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "from sklearn import svm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ComplexClassifier(nn.Module):\n",
    "    def __init__(self, dim_features=128, num_classes = 10):\n",
    "        super(ComplexClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_features, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, num_classes)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)  # No activation, CrossEntropyLoss includes softmax\n",
    "        return x\n",
    "    \n",
    "class NETWORK_F_MLP(nn.Module):\n",
    "    def __init__(self, input_dim = 784, HIDDEN = 200, out_dim = 200, how_many_layers = 2):\n",
    "        super(NETWORK_F_MLP, self).__init__()\n",
    "        self.dim = out_dim\n",
    "        self.many_layer = how_many_layers\n",
    "        \n",
    "        self.fc_list = []\n",
    "        self.bn_list = []\n",
    "        \n",
    "#         self.fc_list.append(nn.Linear(input_dim+20, HIDDEN, bias=True))\n",
    "        self.fc_list.append(nn.Linear(input_dim, HIDDEN, bias=True))\n",
    "        self.bn_list.append(nn.BatchNorm1d(HIDDEN))\n",
    "\n",
    "        for i in range(0, self.many_layer-1):\n",
    "            self.fc_list.append(nn.Linear(HIDDEN, HIDDEN, bias=True))\n",
    "            self.bn_list.append(nn.BatchNorm1d(HIDDEN))\n",
    "            \n",
    "        self.fc_list = nn.ModuleList(self.fc_list)\n",
    "        self.bn_list = nn.ModuleList(self.bn_list)\n",
    "\n",
    "        self.fc_final = nn.Linear(HIDDEN, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        for i in range(0, self.many_layer):\n",
    "            x = self.fc_list[i](x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.bn_list[i](x)\n",
    "        \n",
    "        x = self.fc_final(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class Advanced1DCNN_channel(nn.Module):\n",
    "    def __init__(self, input_channel=1, num_classes=100, input_size=4000):\n",
    "        super(Advanced1DCNN_channel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channel, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "        \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(int(int(int(int(int(input_size/4)/4)/4)/4))*256, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.MLP = NETWORK_F_MLP(input_dim = 128, HIDDEN = 2000, out_dim = num_classes, how_many_layers = 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, channel = x.shape[0], x.shape[1]\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "#         out = torch.sigmoid(out)\n",
    "        out = self.MLP(out)\n",
    "        return out\n",
    "\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        # If the folder does not exist, create it\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "\n",
    "\n",
    "# net = Advanced1DCNN_channel(60, 128, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099216d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './RESULTS_CROSS_SUBJECTS/' already exists.\n",
      "Iteration 100 Losses (Trace, Primary Movements, Sub Movements) -44.773475646972656 0.998828113079071 2.402092218399048\n",
      "Iteration 100 Accuracies (Primary Movements, Sub Movements) 0.604 0.089\n",
      "Iteration 200 Losses (Trace, Primary Movements, Sub Movements) -44.7713623046875 1.0321075916290283 2.3944528102874756\n",
      "Iteration 200 Accuracies (Primary Movements, Sub Movements) 0.604 0.089\n",
      "Iteration 300 Losses (Trace, Primary Movements, Sub Movements) -45.1462287902832 0.9701733589172363 2.4029810428619385\n",
      "Iteration 300 Accuracies (Primary Movements, Sub Movements) 0.604 0.115\n",
      "Iteration 400 Losses (Trace, Primary Movements, Sub Movements) -45.272247314453125 0.982581377029419 2.4152021408081055\n",
      "Iteration 400 Accuracies (Primary Movements, Sub Movements) 0.604 0.115\n",
      "Iteration 500 Losses (Trace, Primary Movements, Sub Movements) -46.43138122558594 0.9324202537536621 2.407449245452881\n",
      "Iteration 500 Accuracies (Primary Movements, Sub Movements) 0.604 0.115\n",
      "Iteration 600 Losses (Trace, Primary Movements, Sub Movements) -46.222984313964844 0.9988797903060913 2.3453359603881836\n",
      "Iteration 600 Accuracies (Primary Movements, Sub Movements) 0.604 0.115\n",
      "Iteration 700 Losses (Trace, Primary Movements, Sub Movements) -46.474822998046875 0.8791137933731079 2.373166561126709\n",
      "Iteration 700 Accuracies (Primary Movements, Sub Movements) 0.604 0.115\n",
      "Iteration 800 Losses (Trace, Primary Movements, Sub Movements) -46.74363708496094 0.9435142278671265 2.3316259384155273\n",
      "Iteration 800 Accuracies (Primary Movements, Sub Movements) 0.604 0.115\n",
      "Iteration 900 Losses (Trace, Primary Movements, Sub Movements) -47.33177947998047 0.9397045373916626 2.303684949874878\n",
      "Iteration 900 Accuracies (Primary Movements, Sub Movements) 0.604 0.115\n",
      "Iteration 1000 Losses (Trace, Primary Movements, Sub Movements) -46.99601364135742 0.838548481464386 2.3291268348693848\n",
      "Iteration 1000 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 1100 Losses (Trace, Primary Movements, Sub Movements) -47.15851593017578 0.8930176496505737 2.3191280364990234\n",
      "Iteration 1100 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 1200 Losses (Trace, Primary Movements, Sub Movements) -47.287498474121094 0.9157273173332214 2.358153820037842\n",
      "Iteration 1200 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 1300 Losses (Trace, Primary Movements, Sub Movements) -48.31566619873047 0.8805446624755859 2.2894577980041504\n",
      "Iteration 1300 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 1400 Losses (Trace, Primary Movements, Sub Movements) -48.48825454711914 0.9287728071212769 2.298557758331299\n",
      "Iteration 1400 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 1500 Losses (Trace, Primary Movements, Sub Movements) -48.07134246826172 0.8595102429389954 2.2558586597442627\n",
      "Iteration 1500 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 1600 Losses (Trace, Primary Movements, Sub Movements) -48.73951721191406 0.8429474830627441 2.2043821811676025\n",
      "Iteration 1600 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 1700 Losses (Trace, Primary Movements, Sub Movements) -48.559932708740234 0.8523618578910828 2.2177271842956543\n",
      "Iteration 1700 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 1800 Losses (Trace, Primary Movements, Sub Movements) -49.69651412963867 0.7796557545661926 2.2409372329711914\n",
      "Iteration 1800 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 1900 Losses (Trace, Primary Movements, Sub Movements) -49.07029342651367 0.7685979604721069 2.239924669265747\n",
      "Iteration 1900 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 2000 Losses (Trace, Primary Movements, Sub Movements) -49.10187530517578 0.7662689685821533 2.2476649284362793\n",
      "Iteration 2000 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 2100 Losses (Trace, Primary Movements, Sub Movements) -49.79142379760742 0.871124804019928 2.2047886848449707\n",
      "Iteration 2100 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n",
      "Iteration 2200 Losses (Trace, Primary Movements, Sub Movements) -49.72941970825195 0.8298617601394653 2.1745595932006836\n",
      "Iteration 2200 Accuracies (Primary Movements, Sub Movements) 0.604 0.125\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "class NETWORK_F_MLP(nn.Module):\n",
    "    def __init__(self, input_dim = 784, HIDDEN = 200, out_dim = 200, how_many_layers = 2):\n",
    "        super(NETWORK_F_MLP, self).__init__()\n",
    "        self.dim = out_dim\n",
    "        self.many_layer = how_many_layers\n",
    "        \n",
    "        self.fc_list = []\n",
    "        self.bn_list = []\n",
    "        \n",
    "#         self.fc_list.append(nn.Linear(input_dim+20, HIDDEN, bias=True))\n",
    "        self.fc_list.append(nn.Linear(input_dim, HIDDEN, bias=True))\n",
    "        self.bn_list.append(nn.BatchNorm1d(HIDDEN))\n",
    "\n",
    "        for i in range(0, self.many_layer-1):\n",
    "            self.fc_list.append(nn.Linear(HIDDEN, HIDDEN, bias=True))\n",
    "            self.bn_list.append(nn.BatchNorm1d(HIDDEN))\n",
    "            \n",
    "        self.fc_list = nn.ModuleList(self.fc_list)\n",
    "        self.bn_list = nn.ModuleList(self.bn_list)\n",
    "\n",
    "        self.fc_final = nn.Linear(HIDDEN, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        for i in range(0, self.many_layer):\n",
    "            x = self.fc_list[i](x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.bn_list[i](x)\n",
    "        \n",
    "        x = self.fc_final(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class Advanced1DCNN_channel(nn.Module):\n",
    "    def __init__(self, input_channel=1, num_classes=100, input_size=4000, num_channel=60):\n",
    "        super(Advanced1DCNN_channel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256 * 15, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.MLP = NETWORK_F_MLP(input_dim = 128*input_channel, HIDDEN = 4000, out_dim = num_classes, how_many_layers = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, channel = x.shape[0], x.shape[1]\n",
    "        x = x.unsqueeze(2)\n",
    "        x = x.flatten(0, 1)\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "#         print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = out.reshape(bs, channel, -1)\n",
    "        out = out.flatten(-2, -1)\n",
    "        out = self.MLP(out)\n",
    "        return out\n",
    "\n",
    "class ComplexClassifier(nn.Module):\n",
    "    def __init__(self, dim_features=128, num_classes = 10):\n",
    "        super(ComplexClassifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(dim_features, 256)\n",
    "#         self.bn1 = nn.BatchNorm1d(256)\n",
    "#         self.fc2 = nn.Linear(256, 512)\n",
    "#         self.bn2 = nn.BatchNorm1d(512)\n",
    "#         self.fc3 = nn.Linear(512, 256)\n",
    "#         self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(dim_features, num_classes)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = torch.relu(self.bn1(self.fc1(x)))\n",
    "#         x = torch.relu(self.bn2(self.fc2(x)))\n",
    "#         x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)  # No activation, CrossEntropyLoss includes softmax\n",
    "        # Using a linear network for channel network  \n",
    "        return x\n",
    "\n",
    "def return_cost_trace(RFG, track_cov_estimate_final):\n",
    "\n",
    "    RF_E = track_cov_estimate_final[:128, :128]\n",
    "    RG_E = track_cov_estimate_final[128:, 128:]\n",
    "    P_E = track_cov_estimate_final[:128, 128:]\n",
    "\n",
    "    RF_EI = torch.inverse(RF_E)\n",
    "    RG_EI = torch.inverse(RG_E)\n",
    "\n",
    "    RF = RFG[:128, :128]\n",
    "    RG = RFG[128:, 128:]\n",
    "    P = RFG[:128, 128:]\n",
    "\n",
    "    COST = -RF_EI@RF@RF_EI@P_E@RG_EI@P_E.T \\\n",
    "            + RF_EI@P@RG_EI@P_E.T \\\n",
    "            - RF_EI@P_E@RG_EI@RG@RG_EI@P_E.T \\\n",
    "            + RF_EI@P_E@RG_EI@P.T\n",
    "    \n",
    "    TSD = RF_EI@P_E@RG_EI@P_E.T\n",
    "\n",
    "    return -torch.trace(COST), -torch.trace(TSD)\n",
    "\n",
    "# for some reasons the adaptive filter is needed\n",
    "def adaptive_estimation(v_t, beta, square_term, i):\n",
    "    v_t = beta*v_t + (1-beta)*square_term.detach()\n",
    "    return v_t, (v_t/(1-beta**i))\n",
    "\n",
    "def MCA_LOSS_GIVEN_R(RP, track_cov, i, dim):\n",
    "    cov = RP + torch.eye((RP.shape[0])).cuda()*(.000001)\n",
    "    track_cov, cov_estimate = adaptive_estimation(track_cov, 0.5, cov, i)\n",
    "\n",
    "    cov_estimate_f = cov_estimate[:dim, :dim]\n",
    "    cov_f = cov[:dim, :dim]\n",
    "\n",
    "    cov_estimate_g = cov_estimate[dim:, dim:]\n",
    "    cov_g = cov[dim:, dim:]\n",
    "\n",
    "    LOSS = (torch.linalg.inv(cov_estimate)*cov).sum() - (torch.linalg.inv(cov_estimate_f)*cov_f).sum() -(torch.linalg.inv(cov_estimate_g)*cov_g).sum()\n",
    "    return track_cov, cov_estimate, LOSS\n",
    "\n",
    "torch.cuda.set_device(3)\n",
    "\n",
    "save_path = './RESULTS_CROSS_SUBJECTS/'\n",
    "create_folder_if_not_exists(save_path)\n",
    "\n",
    "NET_1 = Advanced1DCNN_channel(SAMPLE_X.shape[1], 128, 4000).cuda()\n",
    "NET_2 = Advanced1DCNN_channel(SAMPLE_Y.shape[1], 128, 4000).cuda()\n",
    "\n",
    "classifier_major = ComplexClassifier(dim_features = 128, num_classes = 3).cuda()\n",
    "classifier_subtle = ComplexClassifier(dim_features = 128, num_classes = 11).cuda()\n",
    "classifier_subj = ComplexClassifier(dim_features = 128, num_classes = 25).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "lr1 = 0.0005\n",
    "lr2 = 0.0005\n",
    "\n",
    "optimizer_1 = optim.Adam([\n",
    "    {'params': NET_1.parameters(), 'lr': lr2, 'betas': (beta1, beta2)},\n",
    "], amsgrad = True)\n",
    "\n",
    "optimizer_2 = optim.Adam([\n",
    "    {'params': NET_2.parameters(), 'lr': lr2, 'betas': (beta1, beta2)},\n",
    "], amsgrad = True)\n",
    "\n",
    "optimizer_classifier = optim.Adam([\n",
    "    {'params': classifier_major.parameters(), 'lr': lr2, 'betas': (beta1, beta2)},\n",
    "    {'params': classifier_subtle.parameters(), 'lr': lr2, 'betas': (beta1, beta2)},\n",
    "    {'params': classifier_subj.parameters(), 'lr': lr2, 'betas': (beta1, beta2)},\n",
    "], amsgrad = True)\n",
    "\n",
    "save_curve = []\n",
    "eig_list = []\n",
    "classifier_error = []\n",
    "\n",
    "test_primary_error_save = []\n",
    "test_error_save = []\n",
    "\n",
    "track_cov_final = torch.zeros((128+128)).cuda()\n",
    "track_cov_estimate_final = torch.zeros((128+128)).cuda()\n",
    "\n",
    "for i in range(1, 100000):\n",
    "    batch_size = 100\n",
    "    batch_indices = torch.randint(0, train_X.shape[0], (batch_size,))\n",
    "    input_x = train_X[batch_indices]\n",
    "    input_y = train_Y[batch_indices]\n",
    "    \n",
    "    feature_1 = NET_1(input_x.cuda())\n",
    "    feature_2 = NET_2(input_y.cuda())\n",
    "    \n",
    "    RF = (feature_1.T@feature_1)/feature_1.shape[0]\n",
    "    RG = (feature_2.T@feature_2)/feature_2.shape[0]\n",
    "    P = (feature_1.T@feature_2)/feature_2.shape[0]\n",
    "\n",
    "    \n",
    "    input_dim, output_dim = RF.shape[1], RG.shape[1]\n",
    "    RFG = torch.zeros((input_dim+output_dim, input_dim+output_dim)).cuda()\n",
    "    RFG[:input_dim, :input_dim] = RF\n",
    "    RFG[input_dim:, input_dim:] = RG\n",
    "    RFG[:input_dim, input_dim:] = P\n",
    "    RFG[input_dim:, :input_dim] = P.T\n",
    "    \n",
    "    track_cov_final, track_cov_estimate_final, COST = MCA_LOSS_GIVEN_R(RFG, track_cov_final, i, 128)\n",
    "    COST, TSD = return_cost_trace(RFG, track_cov_estimate_final)\n",
    "    \n",
    "    label_major_batch, label_subtle_batch, label_subj_batch = train_major_label[batch_indices], train_subtle_label[batch_indices], train_subj_label[batch_indices]\n",
    "\n",
    "    output_major, output_subtle, output_subj = classifier_major(feature_1.detach()), classifier_subtle(feature_1.detach()), classifier_subj(feature_1.detach())\n",
    "    loss_1 = criterion(output_major, label_major_batch.cuda())\n",
    "    loss_2 = criterion(output_subtle, label_subtle_batch.cuda())\n",
    "    loss_3 = criterion(output_subj, label_subj_batch.cuda())\n",
    "    (COST+loss_1+loss_2+loss_3).backward()\n",
    "    \n",
    "#     TSD = torch.logdet(track_cov_estimate_final) - torch.logdet(track_cov_estimate_final[:128, :128]) - torch.logdet(track_cov_estimate_final[128:, 128:])\n",
    "    classifier_error.append([TSD.item(), loss_1.item(), loss_2.item(), loss_3.item()])\n",
    "\n",
    "    optimizer_1.step()  \n",
    "    optimizer_2.step()  \n",
    "    optimizer_classifier.step()  \n",
    "\n",
    "    optimizer_1.zero_grad()\n",
    "    optimizer_2.zero_grad()  \n",
    "    optimizer_classifier.zero_grad()\n",
    "        \n",
    "    if i%100 == 0:\n",
    "        print('Iteration', i, 'Losses (Trace, Primary Movements, Sub Movements)', TSD.item(), loss_1.item(), loss_2.item())\n",
    "                \n",
    "        if i%100 == 0:\n",
    "\n",
    "            np.save(save_path+'classifier_error_iter{0}.npy'.format(i), classifier_error)\n",
    "            torch.save(NET_1.state_dict(), save_path+\"NET_1_iter{0}.pth\".format(i))\n",
    "            torch.save(NET_2.state_dict(), save_path+\"NET_2_iter{0}.pth\".format(i))\n",
    "            torch.save(classifier_major.state_dict(), save_path+\"classifier_major_iter{0}.pth\".format(i))\n",
    "            torch.save(classifier_subtle.state_dict(), save_path+\"classifier_subtle_iter{0}.pth\".format(i))\n",
    "            torch.save(classifier_subj.state_dict(), save_path+\"classifier_subj_iter{0}.pth\".format(i))\n",
    "\n",
    "            NET_1.eval()\n",
    "            classifier_major.eval()\n",
    "            classifier_subtle.eval()\n",
    "            classifier_subj.eval()\n",
    "\n",
    "            class_major = []\n",
    "            class_minor = []\n",
    "            class_subj = []\n",
    "\n",
    "            if i%100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    for k in range(0, 1000):\n",
    "\n",
    "            #         for k in range(0, test_X.shape[0], 1):\n",
    "                        output = NET_1(test_X[k:k+1].cuda())\n",
    "                        output_major = classifier_major(output)\n",
    "                        output_subtle = classifier_subtle(output)\n",
    "                        output_subj = classifier_subj(output)\n",
    "\n",
    "                        class_major.append(output_major.mean(0).detach().cpu().numpy())\n",
    "                        class_minor.append(output_subtle.mean(0).detach().cpu().numpy())\n",
    "                        class_subj.append(output_subj.mean(0).detach().cpu().numpy())\n",
    "\n",
    "                    major_accuracy = np.mean(np.stack(class_major).argmax(1) == test_major_label[:np.stack(class_major).argmax(1).shape[0]].numpy())\n",
    "                    subtle_accuracy = np.mean(np.stack(class_minor).argmax(1) == test_subtle_label[:np.stack(class_minor).argmax(1).shape[0]].numpy())\n",
    "                    \n",
    "                    test_primary_error_save.append(major_accuracy.item())\n",
    "                    test_error_save.append(subtle_accuracy.item())\n",
    "\n",
    "                    print('Iteration', i, 'Accuracies (Primary Movements, Sub Movements)', np.array(test_primary_error_save).max(), np.array(test_error_save).max())\n",
    "\n",
    "            NET_1.train()\n",
    "            classifier_major.train()\n",
    "            classifier_subtle.train()\n",
    "            classifier_subj.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
